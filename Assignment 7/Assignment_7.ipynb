{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQDQu9EUyWeP",
        "outputId": "1bb139e8-39b0-4ed6-8596-cc290938a35d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B08tp1RLyaVs",
        "outputId": "29d0715a-63c4-4562-82ab-1ecbeca6a4a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "QjDlanMByfoo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Artificial Intelligence is transforming the world.\n",
        "Students are learning AI and Machine Learning.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bTPpgBETyiKq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAKEQBcvy4-v",
        "outputId": "7c6d449f-a50b-43f9-e306-680e90569980"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKgmJo7jy7so",
        "outputId": "98fa5729-b15f-4904-cb2e-17f943e72d5d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nArtificial Intelligence is transforming the world.', 'Students are learning AI and Machine Learning.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5STF25Aqy-WA",
        "outputId": "2f52ce0c-15eb-407d-ca9a-6e845640ea7a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Artificial', 'Intelligence', 'is', 'transforming', 'the', 'world', '.', 'Students', 'are', 'learning', 'AI', 'and', 'Machine', 'Learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
        "\n",
        "print(\"After Stopword Removal:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU15z7KWzM7n",
        "outputId": "f77cd189-4e3a-4923-936c-45498be07dfc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stopword Removal: ['Artificial', 'Intelligence', 'transforming', 'world', '.', 'Students', 'learning', 'AI', 'Machine', 'Learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRO-QPO5zY7p",
        "outputId": "6bc03ea7-27fc-4de4-9060-ce36a39e5509"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(words)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUhFMxW9zbO8",
        "outputId": "d41f88ad-1fcd-47ca-e406-53dc297255a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('is', 'VBZ'), ('transforming', 'VBG'), ('the', 'DT'), ('world', 'NN'), ('.', '.'), ('Students', 'NNS'), ('are', 'VBP'), ('learning', 'VBG'), ('AI', 'NNP'), ('and', 'CC'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoX4dw0lzj4n",
        "outputId": "a0440d3b-7620-49cf-834e-01cd8f7b20b8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['artifici', 'intellig', 'transform', 'world', '.', 'student', 'learn', 'ai', 'machin', 'learn', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOBXpvKHzl_K",
        "outputId": "95769daf-46d3-4435-fda8-c7233ced4e31"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['Artificial', 'Intelligence', 'transforming', 'world', '.', 'Students', 'learning', 'AI', 'Machine', 'Learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Words:\", len(words))\n",
        "print(\"Unique Words:\", len(set(words)))\n",
        "print(\"Total Sentences:\", len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noBldRPEzm38",
        "outputId": "5fa65f7a-7721-404a-8e99-f02b18193e4f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 15\n",
            "Unique Words: 14\n",
            "Total Sentences: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_analysis(text):\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered = [w for w in words if w.lower() not in stop_words]\n",
        "\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(w) for w in filtered]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
        "\n",
        "    print(\"Words:\", words)\n",
        "    print(\"\\nFiltered:\", filtered)\n",
        "    print(\"\\nPOS Tags:\", pos_tags)\n",
        "    print(\"\\nStemmed:\", stemmed)\n",
        "    print(\"\\nLemmatized:\", lemmatized)\n",
        "    print(\"\\nTotal Words:\", len(words))\n",
        "    print(\"Unique Words:\", len(set(words)))\n",
        "    print(\"Total Sentences:\", len(sentences))"
      ],
      "metadata": {
        "id": "oVsIpc-szosy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_analysis(\"AI is changing the future of technology and education.\")"
      ],
      "metadata": {
        "id": "I5kWVftvzrww",
        "outputId": "dfa5eed7-85d5-45d9-c17c-80ee962d093e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['AI', 'is', 'changing', 'the', 'future', 'of', 'technology', 'and', 'education', '.']\n",
            "\n",
            "Filtered: ['AI', 'changing', 'future', 'technology', 'education', '.']\n",
            "\n",
            "POS Tags: [('AI', 'NNP'), ('is', 'VBZ'), ('changing', 'VBG'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('technology', 'NN'), ('and', 'CC'), ('education', 'NN'), ('.', '.')]\n",
            "\n",
            "Stemmed: ['ai', 'chang', 'futur', 'technolog', 'educ', '.']\n",
            "\n",
            "Lemmatized: ['AI', 'changing', 'future', 'technology', 'education', '.']\n",
            "\n",
            "Total Words: 10\n",
            "Unique Words: 10\n",
            "Total Sentences: 1\n"
          ]
        }
      ]
    }
  ]
}